---
title: "Writing a great story for data science projects"
subtitle: "Summary"
author: "Mika Goins (Advisor: Dr. Seals)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

https://papers.nips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf Links to an external site.
1.	Interpretability vs. Accuracy: While complex models (e.g., deep learning, ensemble methods) often yield higher accuracy, they are harder to interpret. This poses challenges in applications where understanding model decisions is critical.
2.	Unified Framework: SHAP provides an approach to interpreting model predictions by introducing a new class of additive features and important measures. Their are various existing interpretation methods (e.g., LIME, DeepLIFT, Layer-Wise Relevance Propagation) fall within.
3.	Game Theory Foundation: SHAP values are grounded in game theory, specifically the Shapley values, which ensure that the features satisfy properties like local accuracy, missingness, and consistency.

https://jqichina.wordpress.com/wp-content/uploads/2012/02/the-elements-of-statistical-learning.pdf Links to an external site.
Tree-Based Methods:
1•	Decision Trees: The book covers decision tree algorithms, including classification and regression trees (CART). It explains how trees can be used for both classification and regression tasks, highlighting their interpretability and ease of use.
2•	Ensemble Methods: Powerful ensemble methods such as bagging, boosting, and random forests. These techniques combine multiple models to improve predictive performance

https://www.researchgate.net/profile/Gilles-Louppe/publication/264312332_Understanding_Random_Forests_From_Theory_to_Practice/links/54ae38ea0cf2213c5fe427b7/Understanding-Random-Forests-From-Theory-to-Practice.pdf Links to an external site. 

1•	Tree Construction: Each tree in a Random Forest is built using a random subset of features and training data. This randomness helps in making the model robust against over fitting.
2•	Feature Importance: The paper discusses how Random Forests can be used to determine the importance of various features in predicting the target variable. The importance is calculated by looking at how much each feature decreases the impurity in a tree.
3•	Out-of-Bag Error: The out-of-bag error is an unbiased estimate of the model’s prediction error, obtained by using only the samples that were not used in building a particular tree (out-of-bag samples).


