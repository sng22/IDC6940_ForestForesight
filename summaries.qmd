---
title: "Writing a great story for data science projects"
subtitle: "Summary"
author: "Mika Goins (Advisor: Dr. Seals)"
date: '`r Sys.Date(15 September 2024)`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---



# Random Forest in Machine Learning: A Review of Recent Advancements

## Abstract
This paper reviews six influential works in the field of machine learning, with a focus on Random Forest algorithms and their applications. We examine the development of interpretation methods, the foundational concepts of statistical learning, the theoretical underpinnings of Random Forests, and recent empirical studies comparing various classifiers. The review highlights the versatility and effectiveness of Random Forests while also addressing challenges such as bias in variable importance measures.

## 1. Introduction
Machine learning techniques, particularly Random Forest algorithms, have become increasingly important in various domains due to their robustness and versatility. This paper synthesizes findings from six key papers that have contributed significantly to our understanding and application of Random Forests and related techniques in machine learning.

## 2. Methodology
This review examines six papers published between 2001 and 2017, covering theoretical foundations, practical applications, and comparative studies of Random Forests and related machine learning techniques.

## 3. Results and Discussion

### 3.1 Unified Approach to Model Interpretation
(Lundberg and Lee 2017) address the crucial challenge of balancing model complexity with interpretability. They introduce SHAP (SHapley Additive exPlanations), a unified framework for interpreting model predictions. SHAP, grounded in game theory, provides a fair method to distribute the contribution of each feature to the final prediction. This approach ensures that the interpretation method satisfies important properties such as local accuracy, missingness, and consistency. SHAP's ability to encompass various existing interpretation methods allows for a more standardized approach to model interpretation across different types of models and domains. [@Lundberg2017-ww]
  
### 3.2 Foundations of Statistical Learning
(Hastie et al. 2009) provide a comprehensive overview of statistical learning techniques, with a particular focus on tree-based methods. Their work explores decision trees in depth, highlighting their versatility in both classification and regression tasks. The authors also discuss powerful ensemble methods such as bagging, boosting, and random forests, explaining how these techniques combine multiple models to improve predictive performance. This foundational text provides readers with a solid understanding of the advantages and potential drawbacks of these methods. [@Hastie2009-ck]

### 3.3 Theoretical and Practical Aspects of Random Forests
(Louppe 2014) offers a comprehensive examination of Random Forests, covering both theoretical foundations and practical applications. The paper explains the tree construction process in Random Forests, emphasizing the importance of random feature subset selection in building robust models. Louppe also discusses feature importance in Random Forests, explaining how to determine the importance of various features by measuring the decrease in impurity across all trees in the forest. The concept of out-of-bag error is explored as a unique advantage of Random Forests for internal validation.[@Louppe2014-pi]

### 3.4 Introduction and Theoretical Insights into Random Forests
Breiman's seminal paper (Breiman 2001) introduces the Random Forest algorithm, which has since become one of the most popular and effective machine learning techniques. Breiman provides a thorough explanation of the algorithm's foundations, demonstrating how it combines bagging with random feature selection at each node of the decision trees. The paper offers theoretical insights into why Random Forests work so well, discussing the impact of random feature selection on tree correlation and overall forest error rate. Breiman also introduces the concept of "tree strength" and its relationship to forest performance.[@Breiman2001-il]

### 3.5 Comparative Study of Classifiers
(Fern√°ndez-Delgado et al. 2014) present a comprehensive empirical study comparing 179 classifiers from 17 different families across 121 datasets. Their findings strongly support the effectiveness of Random Forests, which consistently rank among the top-performing classifiers across a wide range of problems. The study discusses trade-offs between different classifier families, considering factors such as accuracy, computational efficiency, and ease of use. This extensive comparison offers valuable insights into when Random Forests might be particularly effective and provides guidance for practitioners in choosing appropriate algorithms for specific problems.[@Fernandez-Delgado2014-iz]

### 3.6 Addressing Bias in Random Forest Variable Importance Measures
(Strobl et al. 2007) address a critical issue in the interpretation of Random Forests: bias in variable importance measures. The authors demonstrate that standard variable importance measures in Random Forests can be biased, particularly when predictor variables vary in their scale of measurement or number of categories. To address this issue, they propose a conditional permutation importance measure that accounts for correlations between predictor variables, providing a more accurate assessment of each variable's true importance. This work highlights the need for careful consideration of potential biases in machine learning interpretation techniques.[@Strobl2007-ks]

## 4. Conclusion
This review of six influential papers in the field of machine learning, with a focus on Random Forests, highlights the significant advancements made in both theoretical understanding and practical applications of these techniques. From the development of unified interpretation frameworks to comprehensive empirical studies and the addressing of biases in importance measures, these works collectively demonstrate the power and versatility of Random Forests while also acknowledging the challenges that remain. Future research should continue to build on these foundations, addressing remaining limitations and expanding the application of Random Forests to new domains.





